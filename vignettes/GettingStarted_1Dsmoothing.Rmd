---
title: "Getting Started with MRSea: One dimensional smoothing"
author: "LAS Scott-Hayward"
output: html_document
date: "`r Sys.Date()`"
resource_files:
  - images/MRSea_workflow.png
bibliography: newref.bib
vignette: >
  %\VignetteIndexEntry{Getting Started with MRSea}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---


<!-- **Please reference this document as:** -->
<!-- Scott-Hayward, L.A.S., Mackenzie, M.L. and Walker, C.G. (2021). Vignette for the MRSea Package v1.3: Statistical Modelling of bird and cetacean distributions in offshore renewables development areas. Centre for Research into Ecological and Environmental Modelling, University of St Andrews. -->

<!-- ******* -->

```{r echo=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(fig=TRUE, warning=FALSE, message=FALSE, 
                      eval=TRUE, cache=FALSE,
                      comment = '#>', collapse=TRUE, dev='png')
```

# Introduction

The ```MRSea``` package was developed for analysing data that was collected for assessing potential impacts of renewable developments on marine wildlife, although the methods are applicable to many other studies as well. For example, these methods have been used for more general spatial distribution modelling and to analyse GPS tagging data for home ranges.

The `MRSea` package primarily fits Generalised Additive Models (GAMs) using a spatially adaptive model selection framework for both one and two dimensional covariates using the functions `runSALSA1D` and `runSALSA2D`.  These functions implement the methods of @Walker2010 and @ScottH2023. In addition, options are available for a variety of different splines and the estimation of robust standard errors if residual correlation is present. 

A class of model `gamMRSea` is created when running either SALSA 1D or 2D. This retains within the model object information regarding fitting, such as the `splineParam` object and the panel structure (if present).  The use of the `summary` function on these models returns both raw and robust standard errors, with the *p*-values from the models hypothesis test using the robust standard errors.  The robust standard errors are obtained using the panel structure given (independence is one panel per data point and is the default if no structure is given).

In addition to the functions required to run the models (which we shall go through below) there are also a variety of associated functions for:
Diagnostics:
  - `runACF` (to assess residual correlation), 
  - `runsTest` (to assess residual correlation)
  - `runDiagnostics` (plots of observed vs fitted and fitted vs scaled Pearsons residuals), 
  - `timeInfluenceCheck`/`runInfluence` (assessing the influence of data on precision and predictions), 
  - `plotCumRes` (plots of cumulative residuals)
  
Covariate Checking/Selection:
 - `summary.gamMRSea` (summary function for models)
 - `runPartialPlots` (to plot 1D partial smooth relationships), 
 - `anova.gamMRSea`  (for model selection; (ANOVA) for robust standard errors) and 

Inference:
 - `do.bootstrap.cress.robust` (percentile based confidence intervals).
 - `getDifferences` (identifying differences between two prediction surfaces)



# Fitting a Simple Model

The data we shall use for this example is from a Danish offshore windfarm and is part of the MRSea package. The data are counts of birds collected along transects over a number of surveys and years. In this first example, we will use all of the data together and assess if there is a relationship between number of birds and sea depth. 

```{r message=FALSE, warning=FALSE}
library(dplyr)
library(MRSea)
library(ggplot2)
```

```{r}
# load the data
data("nysted.analysisdata")
wfdata <- filter(nysted.analysisdata, impact==0, season==1)
# load the prediction grid
data("nysted.predictdata")
```


```{r message=FALSE, warning=FALSE}
ggplot() + geom_point(data=wfdata, aes(x=depth, y=response), alpha=1/5) +
  xlab("Sea Depth (m)") + ylab("Number of Birds")
```


## Fitting a 1D smooth

Set up the initial model with  the offset term (if required) and specify the parameters required. Here we add an offset to be the size of the segment associated with the bird counts.  In reality, our bird counts are over a particular area so we have counts per unit area.

```{r message=FALSE}
initialModel <- glm(response ~ 1 + offset(log(area)), family = "quasipoisson", 
                    data = wfdata)
```

The fitness measure can be one of several options (AIC, BIC, QAIC, QBIC, CV). Here we use QBIC as we have a quasi model and information criterion fitting is faster than cross-validation. The default smooth is a B-spline with degree specified as a parameter. 

- `minKnots_1d` specifies the minimum number of internal knots to be selected.
- `maxKnots_1d` specifies the maximum number of internal knots to be selected. Three is usually a good amount of flexibility and a good start point.
- `startKnots_1d` specifies the number of internal knots the SALSA algorithm uses to initialise the process. Usually good to start at 1 knot (this puts one knot at the mean initially)
- `degree` specifies the degree of the B-spline. Two is a quadratic. Three is cubic. 
- `gaps` specifies the minimum gap between knots.  Usually this is fine to be set at zero but occasionally there can be estimation problems and so a gap can be set (in the units of the covariate).


```{r }
salsa1dlist <- list(fitnessMeasure = "QBIC", 
                    minKnots_1d = 1,
                    maxKnots_1d = 3, 
                    startKnots_1d = 1, 
                    degree = 2,
                    gaps = c(0))
```

If you wish to make predictions once the model is fitted, then a prediction grid should be specified in the `runSALSA1D` statement.  This is because the default splines fitted here (B-splines) are unable to make predictions outside of the range they were created.  For example, if the data range for depth is smaller than the range of depths in the prediction data, predictions cannot be made. Here the range of the predictions is slightly wider than the range of the data, so we will specify `nysted.predictdata` when running SALSA.

Run SALSA:

```{r message=FALSE, warning=FALSE, echo=TRUE, results='hide'}
salsa1dOutput <- runSALSA1D(initialModel = initialModel, 
                            salsa1dlist = salsa1dlist,
                            varlist = c("depth"),
                            predictionData = nysted.predictdata, 
                            datain = wfdata,
                            suppress.printout = TRUE)
```

Note that `suppress.printout` will not print the progress of `runSALSA1D` into your workspace but will save the output to a log file (*salsa1d.log*) in your working directory. You may find it helpful to not suppress the print out to begin with so you can see what is happening. 

Use the built in summary function (`summary.gamMRSea`) to look at the summary of the model.  Note that robust standard errors are given alongside the raw standard errors and information regarding panels is at the bottom of the output. If each data point is a panel, then independence is assumed and the two standard error columns will be identical. 

## Model Summary 

The object `salsa1doutput` has four components:

- `bestModel`: the final model
- `modelFits1D`: a list object with an element for each term fitted to the model.  The list includes information on whether the covariate was kept in the model (more on model selection later) and if so the knot locations selected and fitness measure. 
- `fitStat`: The fitness measure of the best model using the fitness measure specified, the cross-validation score of the model (if model selection undertaken) and the dispersion parameter estimate if a quasi model is specified.
- `keptvarlist`: the list of variables retained in the model (same as input if no model selection)


```{r eval=TRUE}
summary(salsa1dOutput$bestModel)
```

You can find the number of knots chosen for a variable by looking at the `modelFits1D` output within `salsa1dOutput` or by querying the spline parameters list in the model object. In this case, one knot has been selected. 

```{r eval=TRUE}
# How many knots were chosen for depth?
salsa1dOutput$bestModel$splineParams[[2]]$knots
```

Assessing model fit using a partial plot. These can be on the link scale:

```{r eval=TRUE, fig=TRUE, fig.align='center', fig.width=6, fig.height=4, message=FALSE}
runPartialPlots(model = salsa1dOutput$bestModel, data = wfdata, 
                varlist = 'depth', 
                showKnots = TRUE, 
                type='link', 
                includeB0 = TRUE)
```

or the response scale

```{r}
runPartialPlots(model = salsa1dOutput$bestModel, data = wfdata, 
                varlist = 'depth', 
                showKnots = TRUE, type='response', 
                includeB0 = TRUE)
```

## Adding a factor variable

The process is the same as above but you specify the factor variable in the initial Model. Here we add a season variable. For this we need to use the full data set `nysted.analysisdata`. 

```{r message=FALSE}
initialModel <- glm(response ~ 1 + as.factor(season) + offset(log(area)), family = "quasipoisson", 
                    data = nysted.analysisdata)
```

```{r }
salsa1dlist <- list(fitnessMeasure = "QBIC", 
                    minKnots_1d = 1,
                    maxKnots_1d = 3, 
                    startKnots_1d = 1, 
                    degree = 2,
                    gaps = c(0))

salsa1dOutput.f <- runSALSA1D(initialModel = initialModel, 
                            salsa1dlist = salsa1dlist,
                            varlist = c("depth"),
                            predictionData = nysted.predictdata, 
                            datain = nysted.analysisdata,
                            suppress.printout = TRUE)
```

```{r eval=TRUE, fig=TRUE, fig.align='center', fig.width=6, fig.height=4, message=FALSE}
runPartialPlots(model = salsa1dOutput.f$bestModel, 
                data = nysted.analysisdata, 
                varlist.in = 'depth', 
                factorlist.in = "season",
                showKnots = TRUE, 
                type='link', 
                includeB0 = TRUE)
```


# Multiple smooth covariates

For this section we will use another renewables data set which is simulated from a vantage point survey of an area used for testing of wave energy devices. 

The data are locations on a grid that are observed for birds at a variety of tide states (Flood, Slack, Ebb), times of day, months and over two years. The first year was baseline data and the second after the installation of a testing device. For this example, we will just look at the baseline data (before impact).

```{r message=FALSE}
data(ns.data.re)
vpdata <- filter(ns.data.re, impact==0) %>%
  mutate(response = birds)
head(vpdata)
```

For the initial model, we include the factor variable for tide state (`flood ebb`) and an offset for the area of a grid cell. 

```{r}
initialModel <- glm(response ~ 1 + as.factor(floodebb) + offset(log(area)), family = "quasipoisson", 
                    data = vpdata)
```

We have picked two variables for inclusion as smooth terms, observation hour and x-coordinate. 

```{r }
varlist <- c("observationhour", "x.pos")

salsa1dlist <- list(fitnessMeasure = "QBIC", 
                    minKnots_1d = rep(1, length(varlist)),
                    maxKnots_1d = rep(1, length(varlist)), 
                    startKnots_1d = rep(1, length(varlist)), 
                    degree = rep(2, length(varlist)),
                    gaps = rep(0, length(varlist)))

salsa1dOutput.multi <- runSALSA1D(initialModel = initialModel, 
                            salsa1dlist = salsa1dlist,
                            varlist = varlist, 
                            datain = vpdata,
                            suppress.printout = TRUE)
```

```{r eval=TRUE, fig=TRUE, fig.align='center', fig.width=6, fig.height=4, message=FALSE}
runPartialPlots(model = salsa1dOutput.multi$bestModel, 
                data = vpdata, 
                varlist.in = varlist,
                factorlist.in = "floodebb",
                showKnots = TRUE, 
                type='link', 
                includeB0 = TRUE)
```

The default is to include these covariates as B-splines.  You might consider that observation hour should be a cyclic spline and there is information [here](https://lindesaysh.github.io/MRSea/article/FittingDifferentSplines_MRSea_v1.3) on how to use cyclic or natural splines.

To include a two dimensional smooth, e.g. a smooth of coordinate space, see the vignette [here](https://lindesaysh.github.io/MRSea/article/GettingStarted_2Dsmoothing)


## Model selection

Fitting the models as described above will always return the non-factor covariates as smooth terms regardless of whether this is sensible or not. There is an optional parameter in `runSALSA1D` which allows selection for each smooth term (smooth, linear or removed) using, by default, 10-fold cross-validation (regardless of fitness measure chosen). To implement this, set the paramater `removal` to be `TRUE`.

Using the same model above with some more covariates added:


```{r}
initialModel <- glm(response ~ 1 + floodebb + offset(log(area)), family = "quasipoisson", 
                    data = vpdata)

varlist <- c("observationhour", "x.pos", "y.pos", "MonthOfYear")

salsa1dlist <- list(fitnessMeasure = "QBIC", 
                    minKnots_1d = rep(1, length(varlist)),
                    maxKnots_1d = rep(1, length(varlist)), 
                    startKnots_1d = rep(1, length(varlist)), 
                    degree = rep(2, length(varlist)),
                    gaps = rep(0, length(varlist)))

salsa1dOutput.multi.rm <- runSALSA1D(initialModel = initialModel, 
                            salsa1dlist = salsa1dlist,
                            varlist = varlist, 
                            datain = vpdata,
                            removal = TRUE, ##
                            suppress.printout = TRUE)
```


In this case we can see that `MonthOfYear` was removed from the model altogether. You can see this from the summary of the model or the `modelFits` output:

```{r}
salsa1dOutput.multi.rm$modelFits[[5]]
```

You can see from this output that the model fit (CV score) increased from the baseModel to the one with the Month term included ad that the term was not kept (and therefore also had no knots selected).


```{r eval=TRUE, fig=TRUE, fig.align='center', fig.width=6, fig.height=4, message=FALSE}
runPartialPlots(model = salsa1dOutput.multi.rm$bestModel, 
                data = vpdata, 
                varlist.in = salsa1dOutput.multi.rm$keptvarlist,  ##
                factorlist.in = "floodebb",
                showKnots = TRUE, 
                type='link', 
                includeB0 = TRUE)
```

In an ideal world you should use cross-validation for both the flexibility selection and model term selection. However, using CV can be computationally expensive and so using an information criterion for flexibility selection and then CV for variable selection can be more efficient. 

What about the factor variable? Factor variable selection is not currently included as part of the SALSA1D variable selection procedure.  To check this you can manually assess a model with this term removed. 

```{r}
fit_rmfloodeb <- update(salsa1dOutput.multi.rm$bestModel, . ~ . - floodebb)

set.seed(123)
cv.gamMRSea(modelobject = salsa1dOutput.multi.rm$bestModel, 
            data = vpdata, 
            K = 10)$delta[2]
set.seed(123)
cv.gamMRSea(modelobject = fit_rmfloodeb, 
            data = vpdata, 
            K = 10)$delta[2]

```

The CV score increases when the tide state variable is removed so we choose to retain it in the model. 

### Other options

Alternatively, you can use $p$-value selection via an F-test ANOVA.

```{r}
anova(salsa1dOutput.multi.rm$bestModel)
```
By specifying `anova` it is using the `anova.gamMRSea` function (as the class of the model is a `gamMRSea` model) which uses marginal testing and, if available, will use a robust variance-covariance matrix for testing.

# Further information:

For information on:

1. [Other types of 1D spline](https://lindesaysh.github.io/MRSea/article/FittingDifferentSplines_MRSea_v1.3))
2. [Two dimensional smoothing](https://lindesaysh.github.io/MRSea/article/GettingStarted_2Dsmoothing)
3. [Full Case Study](https://lindesaysh.github.io/MRSea/article/RenewablesCaseStudy_MRSea_v1.3) including information on distance sampling, `gamMRSea` model diagnostics and using robust standard errors to deal with residual correlation. 

<!-- ## Selection of flexibility for 2D smooth term -->

<!-- 9. Create a grid of knots that will be used as possible knot locations.  This may take while and could be different every time you run it so I suggest saving the knotgrid as a file. -->

<!-- ```{r knotgrid, fig=TRUE, fig.align='center', fig.width=9, fig.height=6} -->
<!-- knotgrid<- getKnotgrid(coordData = cbind(count.data$x.pos, count.data$y.pos),  -->
<!--                        numKnots = 300, -->
<!--                        plot = FALSE) -->
<!-- # -->
<!-- # write.csv(knotgrid, file='knotgrid_fullanalysis.csv', row.names=F) -->
<!-- # ~~~~~~~~~~~~~~~~~~~~~~~ -->
<!-- ``` -->

<!-- 10. Set up parameters for SALSA2D.  Distance matrices (data to knots and knot to knots), a fit statistic and min, max and start knots. -->

<!-- ```{r } -->
<!-- # make distance matrices for datatoknots and knottoknots -->
<!-- distMats <- makeDists(cbind(count.data$x.pos, count.data$y.pos), knotgrid) -->

<!-- # ~~~~~~~~~~~~~~~~~~~~~~~ -->

<!-- # make parameter set for running salsa2d -->
<!-- salsa2dlist<-list(fitnessMeasure = 'QBIC',  -->
<!--                   knotgrid = knotgrid, -->
<!--                   startKnots=10,  -->
<!--                   minKnots=4,  -->
<!--                   maxKnots=15,  -->
<!--                   gap=0,  -->
<!--                   interactionTerm="as.factor(impact)") -->
<!-- ``` -->

<!-- 11. Run SALSA2D to find the appropriate number and location of knots for the 2D smooth term of `x.pos` and `y.pos`. The model inputted to the SALSA algorithm is the model output from the 1D SALSA run.  If you have no univariate smooth terms, you can put in the initial model in this step.  -->

<!-- ```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'} -->
<!-- salsa2dOutput<-runSALSA2D(model = salsa1dOutput$bestModel,  -->
<!--                           salsa2dlist = salsa2dlist,  -->
<!--                           d2k=distMats$dataDist, -->
<!--                           k2k=distMats$knotDist) -->
<!-- ``` -->


<!-- ```{r eval=TRUE} -->
<!-- require(ggplot2) -->
<!-- ggplot() + geom_point(data=count.data, aes(x=x.pos, y=y.pos), colour='grey') + -->
<!--   theme_bw() +  -->
<!--   geom_point(data=data.frame(knotgrid), aes(X1, X2), col='blue') +  -->
<!--   geom_point(data=data.frame(knotgrid)[salsa2dOutput$aR[[1]],],  -->
<!--              aes(X1, X2), col='darkgreen', size=4) + -->
<!--   coord_equal() -->

<!-- ``` -->

<!-- 12.  Are the residuals correlated? Make a suitable blocking structure, within which residuals are expected to be correlated but between which they are independent.  Use `runACF` to assess the blocking structure. -->

<!-- ```{r eval=TRUE, acfplot, fig.cap='ACF plot showing correlation in each block (grey lines), and the mean correlation by lag across blocks (red line).'} -->
<!-- runACF(block = count.data$blockid, model = salsa2dOutput$bestModel, -->
<!--        suppress.printout=TRUE) -->
<!-- ``` -->

<!-- Here we also do a runs test to assess for correlation in the model residuals.  Since our data are over-dispersed, we must use the empirical distribution for assessment: -->

<!-- ```{r} -->
<!-- simData<-generateNoise(n=500, response=fitted(salsa2dOutput$bestModel), family='poisson', d=summary(salsa2dOutput$bestModel)$dispersion) -->
<!-- empdist<-getEmpDistribution(500, simData, salsa2dOutput$bestModel, data=count.data,dots=FALSE) -->
<!-- ``` -->

<!-- ```{r eval=TRUE} -->
<!-- runsTest(residuals(salsa2dOutput$bestModel, type='pearson'),emp.distribution=empdist) -->
<!-- ``` -->


<!-- 13. Model selection -->

<!-- ```{r eval=TRUE} -->
<!-- set.seed(1) -->
<!-- # 2D model -->
<!-- cv.gamMRSea(data=count.data, modelobject = salsa2dOutput$bestModel, K=10)$delta[2] -->
<!-- # salsa2dOutput$fitStat # alternatively -->

<!-- # 1D model -->
<!-- cv.gamMRSea(data=count.data, modelobject = salsa1dOutput$bestModel, K=10)$delta[2] -->

<!-- # intitial model -->
<!-- cv.gamMRSea(data=count.data, modelobject = initialModel, K=10)$delta[2] -->

<!-- ``` -->

<!-- ```{r eval=TRUE} -->
<!-- anova(salsa2dOutput$bestModel) -->
<!-- ``` -->

<!-- ```{r eval=TRUE, fig=TRUE, fig.align='center', fig.width=6, fig.height=4, message=FALSE} -->
<!-- par(mfrow=c(2,2)) -->
<!-- runPartialPlots(model = salsa2dOutput$bestModel, data = count.data,  -->
<!--                 factorlist.in = c('season', 'impact'),  -->
<!--                 varlist.in = 'depth', showKnots = T,  -->
<!--                 includeB0 = TRUE) -->
<!-- ``` -->

<!-- # ```{r eval=TRUE, fig=TRUE, fig.align='center', fig.width=6, fig.height=4, message=FALSE} -->
<!-- # par(mfrow=c(2,2)) -->
<!-- # runPartialPlots(model = salsa2dOutput$bestModel, data = count.data,  -->
<!-- #                 factorlist = c('season', 'impact'), varlist = 'depth',  -->
<!-- #                 showKnots = T, type='link',  -->
<!-- #                 includeB0 = TRUE) -->
<!-- # ``` -->
<!-- #  -->
<!-- # ## Making Predictions -->

<!-- ```{r } -->
<!-- preddist<-makeDists(cbind(predictData$x.pos, predictData$y.pos),  -->
<!--                  knotgrid, knotmat=FALSE)$dataDist -->


<!-- # make predictions on response scale -->
<!-- preds<-predict(newdata = predictData,  -->
<!--                g2k = preddist,  -->
<!--                object = salsa2dOutput$bestModel) -->
<!-- ``` -->

<!-- Plotting the predictions pre and post impact: -->

<!-- ```{r } -->
<!-- require(RColorBrewer) -->
<!-- imp.labs <- c("Pre-Construction", "Post-Construction") -->
<!-- names(imp.labs) <- c("0", "1") -->
<!-- predictData$preds<-preds[,1] -->
<!-- ``` -->

<!-- ```{r eval=TRUE, fig=TRUE, fig.align='center', fig.height=10, fig.width=8} -->
<!-- ggplot() +  -->
<!--   geom_tile(data=predictData, aes(x.pos, y.pos, fill=preds), height=0.5, width=0.5) + -->
<!--   facet_grid(season~impact , labeller = labeller(impact=imp.labs)) + -->
<!--   theme_bw() + coord_equal() +  -->
<!--   scale_fill_distiller(palette = "Spectral",name="Animal Counts") -->
<!-- ``` -->

<!-- ## Bootstrapped Confidence Intervals and Difference Surfaces -->

<!-- Note that the coding in this section has changed slightly from the original user guide. -->

<!-- 14. Bootstrap to include parameter estimation uncertainty in the detection function and parameter estimation in the spatial model. (Note: If no detection function estimated, then the bootstrap is just on the parameters of the spatial model.) -->

<!-- ```{r boots, warning=FALSE, message=FALSE, results='hide'} -->
<!-- dis.data$seasonimpact <- paste(dis.data$season, dis.data$impact) -->

<!-- bootPreds<-do.bootstrap.cress.robust(model.obj = salsa2dOutput$bestModel,  -->
<!--                                      predictionGrid = predictData, -->
<!--                                      g2k=preddist, -->
<!--                                      B = 100,  -->
<!--                                      robust=TRUE) -->
<!-- ``` -->

<!-- ```{r } -->
<!-- #load('predictionboot.RData') -->
<!-- cis <- makeBootCIs(bootPreds) -->
<!-- ``` -->

<!-- 15. Calculate the differences before and after across all bootstraps -->
<!-- ```{r } -->
<!-- differences <- getDifferences(beforePreds =  -->
<!--                       bootPreds[predictData$impact == 0, ], -->
<!--                       afterPreds = bootPreds[predictData$impact == 1, ]) -->
<!-- ``` -->

<!-- 16. Plot differences and indicate where significant positive/negative differences lie.  The grey circles indicate a significant negative difference (abundance after impact is less than the abundance before impact) and the grey crosses indicate a significant positive difference.  The colour of the cell indicates the size of the difference.  -->

<!-- ```{r eval=TRUE, fig=TRUE, fig.align='center', fig.width=9, fig.height=6} -->
<!-- mediandiff <- differences$mediandiff -->
<!-- # The marker for each after - before difference: -->
<!-- # positive ('1') and negative ('-') significant differences -->
<!-- marker <- differences$significanceMarker -->
<!-- par(mfrow = c(1, 1)) -->
<!-- quilt.plot(predictData$x.pos[predictData$impact == 0],  -->
<!--            predictData$y.pos[predictData$impact == 0], -->
<!--            mediandiff, asp = 1, nrow = 104, ncol = 55) -->
<!-- # add + or - depending on significance of cells. Just -->
<!-- # requires one significance out of all to be allocated -->
<!-- points(predictData$x.pos[predictData$impact == 0][marker == 1], -->
<!--        predictData$y.pos[predictData$impact == 0][marker == 1], -->
<!--        pch = "+", col = "darkgrey", cex = 0.75) -->
<!-- points(predictData$x.pos[predictData$impact == 0][marker == (-1)], -->
<!--        predictData$y.pos[predictData$impact == 0][marker == (-1)], -->
<!--        col = "darkgrey", cex = 0.75) -->
<!-- points(681417.3/1000, 6046910/1000, cex = 3, pch = "*", lwd = 1, col = "grey") -->
<!-- ``` -->

<!-- Select a single season to plot (here I have chosen season 1): -->

<!-- ```{r eval=TRUE, fig.width=9, fig.height=6} -->
<!-- require(dplyr) -->
<!-- diffdata<-data.frame(predictData[predictData$impact==0,], mediandiff, marker) -->
<!-- diffdata_s1<-filter(diffdata, season==1) -->

<!-- wf<-data.frame(x=(681417.3/1000), y= (6046910/1000)) -->

<!-- ggplot() + geom_tile(data=diffdata_s1, aes(x=x.pos, y=y.pos, fill=mediandiff),  -->
<!--                      height=0.5, width=0.5) +  -->
<!--   geom_point(data=filter(diffdata_s1, marker == 1), aes(x=x.pos, y=y.pos),  -->
<!--              shape=3, colour='darkgrey', size=1) + -->
<!--   geom_point(data=filter(diffdata_s1, marker == -1), aes(x=x.pos, y=y.pos),  -->
<!--              shape=1, colour='darkgrey', size=1.5) + -->
<!--   theme_bw() + coord_equal() +   -->
<!--   scale_fill_distiller(palette = "Spectral",name="Difference") +  -->
<!--   geom_point(data=wf, aes(x, y), shape=8, size=4) -->

<!-- ``` -->

<!-- Or, all seasons (remember season is only in as a factor variable so there will be no change in spatial distribution between seasons, only an increase or decrease in numbers): -->

<!-- ```{r, eval=TRUE, fig.width=9, fig.height=7} -->
<!-- ggplot() + geom_tile(data=diffdata, aes(x=x.pos, y=y.pos, fill=mediandiff),  -->
<!--                      height=0.5, width=0.5) +  -->
<!--   geom_point(data=filter(diffdata, marker == 1), aes(x=x.pos, y=y.pos),  -->
<!--              shape=3, colour='darkgrey', size=1) + -->
<!--   geom_point(data=filter(diffdata, marker == -1), aes(x=x.pos, y=y.pos),  -->
<!--              shape=1, colour='darkgrey', size=1.5) + -->
<!--   theme_bw() + coord_equal() + facet_wrap(~season) +  -->
<!--   scale_fill_distiller(palette = "Spectral",name="Difference") +  -->
<!--   geom_point(data=wf, aes(x, y), shape=8, size=4) -->
<!-- ``` -->

<!-- ## Other useful functions in the MRSea package -->

<!-- I will use the best model above to run some additional diagnostics. -->

<!-- ```{r} -->
<!-- finalmod<-salsa2dOutput$bestModel -->
<!-- ``` -->

<!-- ### `runDiagnostics` -->

<!-- This function creates observed vs fitted and fitted vs scaled pearsons residual plots. -->

<!-- ```{r, eval=TRUE, fig.width=9, fig.height=6} -->
<!-- runDiagnostics(finalmod) -->
<!-- ``` -->

<!-- ### Influence diagnostics -->

<!-- These functions assess the influence of different blocks on the data.  -->

<!-- ```{r eval=TRUE} -->
<!-- timeInfluenceCheck(finalmod, id = count.data$blockid) -->
<!-- ``` -->

<!-- ```{r, eval=TRUE, fig.width=9, fig.height=6} -->
<!-- inflpoints<-runInfluence(finalmod, id = count.data$blockid) -->
<!-- ``` -->

<!-- ### Cumulative residual plots -->

<!-- Cumulative residual plots are returned for residuals ordered by each covariate in `varlist`, predicted value and index of observations (temporally). The blue dots are the residuals and the black line is the line of cumulative residual. On the covariate plots (those in `varlist`) the grey line indicates what we would expect from a well fitted covariate. i.e. one that is fitted with excessive knots. -->

<!-- ```{r,eval=TRUE, fig.width=9, fig.height=7} -->
<!-- plotCumRes(model = finalmod, varlist = 'depth') -->
<!-- ``` -->


<!-- ### Making any `glm` model into an `gamMRSea` object -->

<!-- This can be done using the `make.gamMRSea` function.  Here we use the `fullmodel` which is a `glm` model with `family='poisson'` -->

<!-- ```{r eval=TRUE} -->
<!-- fullModel$call -->
<!-- ``` -->


<!-- ```{r} -->
<!-- fullModel.gamMRSea <- make.gamMRSea(fullModel,   -->
<!--                                     gamMRSea = TRUE) -->

<!-- ``` -->

<!-- ```{r eval=TRUE} -->
<!-- summary(fullModel.gamMRSea) -->
<!-- ``` -->

<!-- Additionally, if you choose to fit the SALSA or GLM models without a panel structure, the `make.gamMRSea` function can be used to add a panel structure afterwards. -->

<!-- ```{r, eval=FALSE} -->
<!-- finalmod.robustse <- make.gamMRSea(finalmod,   -->
<!--                                     gamMRSea = TRUE, -->
<!--                                     panelid = count.data$blockid) -->
<!-- ``` -->

<!-- ```{r} -->
<!-- summary(finalmod.robustse) -->
<!-- ``` -->




<!-- ```{r, eval=FALSE, echo=FALSE, include=FALSE} -->
<!-- save.image(file='mrseavignetteworkspace.RData', compress="bzip2") -->
<!-- ``` -->
